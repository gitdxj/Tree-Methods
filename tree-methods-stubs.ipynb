{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "... # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base classes\n",
    "\n",
    "class Node:\n",
    "    pass\n",
    "\n",
    "class Tree:\n",
    "    def __init__(self):\n",
    "        self.root = Node()\n",
    "    \n",
    "    def find_leaf(self, x):\n",
    "        node = self.root\n",
    "        while hasattr(node, \"feature\"):\n",
    "            j = node.feature\n",
    "            if x[j] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DensityTree(Tree):\n",
    "    def __init__(self):\n",
    "        super(DensityTree, self).__init__()\n",
    "        \n",
    "    def train(self, data, prior, n_min=20):\n",
    "        '''\n",
    "        data: the feature matrix for the digit under consideration\n",
    "        prior: the prior probability of this digit\n",
    "        n_min: termination criterion (don't split if a node contains fewer instances)\n",
    "        '''\n",
    "        self.prior = prior\n",
    "        N, D = data.shape\n",
    "        D_try = int(np.sqrt(D)) # number of features to consider for each split decision\n",
    "\n",
    "        # find and remember the tree's bounding box, \n",
    "        # i.e. the lower and upper limits of the training feature set\n",
    "        m, M = np.min(data, axis=0), np.max(data, axis=0)\n",
    "        self.box = m.copy(), M.copy()\n",
    "        \n",
    "#         print(\"shape of m and M\", m.shape, M.shape)\n",
    "        \n",
    "        # identify invalid features and adjust the bounding box\n",
    "        # (If m[j] == M[j] for some j, the bounding box has zero volume, \n",
    "        #  causing divide-by-zero errors later on. We must exclude these\n",
    "        #  features from splitting and adjust the bounding box limits \n",
    "        #  such that invalid features have no effect on the volume.)\n",
    "        valid_features   = np.where(m != M)[0]\n",
    "        invalid_features = np.where(m == M)[0]\n",
    "        M[invalid_features] = m[invalid_features] + 1\n",
    "        \n",
    "#         print(\"shape of valid features\", valid_features.shape)\n",
    "\n",
    "        # initialize the root node\n",
    "        self.root.data = data\n",
    "        self.root.box = m.copy(), M.copy()\n",
    "\n",
    "        # build the tree\n",
    "        stack = [self.root]\n",
    "        while len(stack):\n",
    "            node = stack.pop()\n",
    "            n = node.data.shape[0] # number of instances in present node\n",
    "            if n >= n_min:\n",
    "                # Call 'make_density_split_node()' with 'D_try' randomly selected \n",
    "                # indices from 'valid_features'. This turns 'node' into a split node\n",
    "                # and returns the two children, which must be placed on the 'stack'.\n",
    "                left, right = make_density_split_node(node, N, np.random.permutation(valid_features)[:D_try])\n",
    "                if left==None or right==None:\n",
    "                    make_density_leaf_node(node, N)\n",
    "                    if len(stack):\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "                stack.append(left)\n",
    "                stack.append(right)\n",
    "            else:\n",
    "                # Call 'make_density_leaf_node()' to turn 'node' into a leaf node.\n",
    "                make_density_leaf_node(node, N)\n",
    "\n",
    "    def predict(self, x):        \n",
    "        # return p(x | y) * p(y) if x is within the tree's bounding box \n",
    "        # and return 0 otherwise\n",
    "        leaf = self.find_leaf(x)\n",
    "        m, M = self.root.box\n",
    "        if np.sum(x<m)>0 or np.sum(x>M)>0:  # out of the bounding box\n",
    "            return 0\n",
    "        else:\n",
    "            return self.prior * leaf.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_density_split_node(node, N, feature_indices):\n",
    "    '''\n",
    "    node: the node to be split\n",
    "    N:    the total number of training instances for the current class\n",
    "    feature_indices: a numpy array of length 'D_try', containing the feature \n",
    "                     indices to be considered in the present split\n",
    "    '''\n",
    "    n, D = node.data.shape\n",
    "    m, M = node.box\n",
    "\n",
    "    # find best feature j (among 'feature_indices') and best threshold t for the split\n",
    "    e_min = float(\"inf\")\n",
    "    j_min, t_min = None, None\n",
    "    \n",
    "    for j in feature_indices:\n",
    "        # Hint: For each feature considered, first remove duplicate feature values using \n",
    "        # 'np.unique()'. Describe here why this is necessary.\n",
    "        \"\"\"if we don't use np.unique(), we will end up calculating the error of some identical thresholds for multiple times\"\"\"\n",
    "        data_unique = np.sort(np.unique(node.data[:, j]))\n",
    "        # Compute candidate thresholds\n",
    "        tj = np.array([(data_unique[i]+data_unique[i+1])/2 for i in range(len(data_unique)-1)])\n",
    "#         print(tj)\n",
    "        # Illustration: for loop - hint: vectorized version is possible\n",
    "        for t in tj:\n",
    "            # Compute the error\n",
    "            Nl = np.sum(node.data[:, j]<=t)\n",
    "            Nr = np.sum(node.data[:, j]>t)\n",
    "#             Vl = np.prod(np.max(node.data[node.data[:, j]<=t], axis=0) - np.min(node.data[node.data[:, j]<=t], axis=0))\n",
    "#             Vr = np.prod(np.max(node.data[node.data[:, j]>t], axis=0) - np.min(node.data[node.data[:, j]>t], axis=0))\n",
    "            V = np.prod(M-m)\n",
    "            Vl = V*(t - m[j])/(M[j] - m[j])\n",
    "            Vr = V*(M[j] - t)/(M[j] - m[j])\n",
    "            loo_error = cal_loo_error(N, Nl, Vl) + cal_loo_error(N, Nr, Vr)\n",
    "            \n",
    "            # choose the best threshold that\n",
    "            if loo_error < e_min:\n",
    "                e_min = loo_error\n",
    "                j_min = j\n",
    "                t_min = t\n",
    "    \n",
    "    if j_min == None:\n",
    "        return None, None\n",
    "    \n",
    "    # create children\n",
    "    left = Node()\n",
    "    right = Node()\n",
    "    \n",
    "    # initialize 'left' and 'right' with the data subsets and bounding boxes\n",
    "    # according to the optimal split found above\n",
    "    left.data = node.data[node.data[:, j_min]<=t_min] # store data in left node -- for subsequent splits\n",
    "    left.box = m.copy(), M.copy()\n",
    "    left.box[1][j_min] = t_min   # store bounding box in left node\n",
    "    right.data = node.data[node.data[:, j_min]>t_min]\n",
    "    right.box = m.copy(), M.copy()\n",
    "    right.box[0][j_min] = t_min\n",
    "\n",
    "    # turn the current 'node' into a split node\n",
    "    # (store children and split condition)\n",
    "    node.left = left\n",
    "    node.right = right\n",
    "    node.feature = j_min\n",
    "    node.threshold = t_min\n",
    "\n",
    "    # return the children (to be placed on the stack)\n",
    "    return left, right\n",
    "\n",
    "\n",
    "def cal_loo_error(N, Nm, Vm):\n",
    "    return (Nm/(N*Vm))*(Nm/N - 2*(Nm-1)/(N-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_density_leaf_node(node, N):\n",
    "    '''\n",
    "    node: the node to become a leaf\n",
    "    N:    the total number of training instances for the current class\n",
    "    '''\n",
    "    # compute and store leaf response\n",
    "    n = node.data.shape[0]\n",
    "    v = np.prod(node.box[1]-node.box[0])\n",
    "    node.response = n/(N*v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DecisionTree(Tree):\n",
    "    def __init__(self):\n",
    "        super(DecisionTree, self).__init__()\n",
    "        \n",
    "    def train(self, data, labels, n_min=20):\n",
    "        '''\n",
    "        data: the feature matrix for all digits\n",
    "        labels: the corresponding ground-truth responses\n",
    "        n_min: termination criterion (don't split if a node contains fewer instances)\n",
    "        '''\n",
    "        N, D = data.shape\n",
    "        D_try = int(np.sqrt(D)) # how many features to consider for each split decision\n",
    "\n",
    "        # initialize the root node\n",
    "        self.root.data = data\n",
    "        self.root.labels = labels\n",
    "        \n",
    "        stack = [self.root]\n",
    "        while len(stack):\n",
    "            node = stack.pop()\n",
    "            n = node.data.shape[0] # number of instances in present node\n",
    "            if n >= n_min and not node_is_pure(node):\n",
    "                # Call 'make_decision_split_node()' with 'D_try' randomly selected \n",
    "                # feature indices. This turns 'node' into a split node\n",
    "                # and returns the two children, which must be placed on the 'stack'.\n",
    "                left, right = make_decision_split_node(node, np.random.permutation(node.data.shape[1])[:D_try])\n",
    "                if left==None or right==None:\n",
    "                    make_decision_leaf_node(node, N)\n",
    "                    if len(stack):\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "                stack.append(left)\n",
    "                stack.append(right)\n",
    "                ... # your code here\n",
    "            else:\n",
    "                # Call 'make_decision_leaf_node()' to turn 'node' into a leaf node.\n",
    "                make_decision_leaf_node(node)\n",
    "                ... # your code here\n",
    "                \n",
    "    def predict(self, x):\n",
    "        leaf = self.find_leaf(x)\n",
    "        # compute p(y | x)\n",
    "        return leaf.response # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_decision_split_node(node, feature_indices):\n",
    "    '''\n",
    "    node: the node to be split\n",
    "    feature_indices: a numpy array of length 'D_try', containing the feature \n",
    "                     indices to be considered in the present split\n",
    "    '''\n",
    "    n, D = node.data.shape\n",
    "\n",
    "    # find best feature j (among 'feature_indices') and best threshold t for the split\n",
    "    e_min = float(\"inf\")\n",
    "    j_min, t_min = None, None\n",
    "    \n",
    "    for j in feature_indices:\n",
    "        data_unique = np.sort(np.unique(node.data[:, j]))\n",
    "        # Compute candidate thresholds\n",
    "        tj = np.array([(data_unique[i]+data_unique[i+1])/2 for i in range(len(data_unique)-1)])\n",
    "        \n",
    "        # Illustration: for loop - hint: vectorized version is possible\n",
    "        for t in tj:\n",
    "            # Compute the error\n",
    "            Nl = np.sum(node.data[:, j]<=t)\n",
    "            Nr = np.sum(node.data[:, j]>t)\n",
    "            gini = cal_gini(Nl, node.labels, node.data[:, j]<=t) + cal_gini(Nr, node.labels, node.data[:, j]>t)\n",
    "            \n",
    "            # choose the best threshold that\n",
    "            if gini < e_min:\n",
    "                e_min = gini\n",
    "                j_min = j\n",
    "                t_min = t\n",
    "    \n",
    "    if j_min == None:\n",
    "        return None, None\n",
    "\n",
    "    # create children\n",
    "    left = Node()\n",
    "    right = Node()\n",
    "    \n",
    "    # initialize 'left' and 'right' with the data subsets and labels\n",
    "    # according to the optimal split found above\n",
    "    left.data = node.data[node.data[:, j_min]<=t_min] # data in left node\n",
    "    left.labels = node.labels[node.data[:, j_min]<=t_min] # corresponding labels\n",
    "    right.data = node.data[node.data[:, j_min]>t_min]\n",
    "    right.labels = node.labels[node.data[:, j_min]>t_min]\n",
    "\n",
    "\n",
    "    # turn the current 'node' into a split node\n",
    "    # (store children and split condition)\n",
    "    node.left = left\n",
    "    node.right = right\n",
    "    node.feature = j_min\n",
    "    node.threshold = t_min\n",
    "\n",
    "    # return the children (to be placed on the stack)\n",
    "    return left, right    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_decision_leaf_node(node):\n",
    "    '''\n",
    "    node: the node to become a leaf\n",
    "    '''\n",
    "    \n",
    "    # compute and store leaf response\n",
    "    node.N = len(node.labels)\n",
    "    response = dict()   # Key: label  Value: K/N\n",
    "    for label in node.labels:\n",
    "        if label not in response:\n",
    "            response[label] = 1\n",
    "        else:\n",
    "            response[label] += 1\n",
    "    for k in response:\n",
    "        response[k] = response[k] / node.N\n",
    "    node.response = response # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def node_is_pure(node):\n",
    "    '''\n",
    "    check if 'node' ontains only instances of the same digit\n",
    "    '''\n",
    "    return len(np.unique(node.labels))==1 # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_gini(Nl, labels, indices):\n",
    "    \"\"\"\n",
    "    Calculation of Gini impurity\n",
    "    Args:\n",
    "        Nl: number of instances in node l\n",
    "        labels: labels of node l's parent\n",
    "        indices: use labels[indices] to get the labels of node l\n",
    "    \"\"\"\n",
    "    label_count = dict()  # key: label  value: count of the label\n",
    "    for label in labels[indices]:\n",
    "        if label not in label_count:\n",
    "            label_count[label] = 1\n",
    "        else:\n",
    "            label_count[label] += 1\n",
    "    gini = 0\n",
    "    for _, Nlk in label_count.items():\n",
    "        gini += np.square(Nlk/Nl)\n",
    "    gini = Nl*(1-gini)\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Density and Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'target_names', 'images', 'DESCR'])\n",
      "(1797, 8, 8)\n",
      "(1797,)\n"
     ]
    }
   ],
   "source": [
    "# read and prepare the digits data\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print(digits.keys())\n",
    "print(digits.images.shape)\n",
    "print(digits.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_err(mat):\n",
    "    assert mat.shape[0] == mat.shape[1]\n",
    "    D = mat.shape[0]\n",
    "    N = np.sum(np.sum(mat, 0))  # number of instances\n",
    "    correct_pred_count = 0\n",
    "    for i in range(D):\n",
    "        correct_pred_count += mat[i][i]\n",
    "    return 1- correct_pred_count/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# train trees, plot training error confusion matrices, and comment on your results\n",
    "def test_density_tree(n_min = 20):\n",
    "    density_trees = [DensityTree() for _ in range(10)]\n",
    "    for i in range(10):\n",
    "        density_trees[i].train(data=digits.data[digits.target==i], prior=np.sum(digits.target==i)/len(digits.target), n_min=n_min)\n",
    "    confusion_matrice = np.zeros((10, 10), dtype=int)\n",
    "    for i in range(len(digits.target)):\n",
    "        x = digits.data[i]\n",
    "        real_label = digits.target[i]\n",
    "        pred_label = np.argsort([density_trees[i].predict(x) for i in range(10)])[-1]\n",
    "        confusion_matrice[real_label][pred_label] += 1\n",
    "    print(\"Density Tree with n_min={}\".format(n_min))\n",
    "    print(confusion_matrice)\n",
    "    print(\"error rate:\", cal_err(confusion_matrice))\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_decision_tree(n_min = 20):\n",
    "    decision_tree = DecisionTree()\n",
    "    decision_tree.train(data=digits.data, labels=digits.target, n_min=n_min)\n",
    "    confusion_matrice = np.zeros((10, 10), dtype=int)\n",
    "    for i in range(len(digits.target)):\n",
    "        x = digits.data[i]\n",
    "        real_label = digits.target[i]\n",
    "        pred_label = 0\n",
    "        max_prob = 0\n",
    "        for label, p in decision_tree.predict(x).items():\n",
    "            if p > max_prob:\n",
    "                pred_label = label\n",
    "                max_prob = p\n",
    "        confusion_matrice[real_label][pred_label] += 1\n",
    "    print(\"Decision Tree with n_min={}\".format(n_min))\n",
    "    print(confusion_matrice)\n",
    "    print(\"error rate:\", cal_err(confusion_matrice))\n",
    "    print(\"\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density Tree with n_min=40\n",
      "[[177   0   0   0   1   0   0   0   0   0]\n",
      " [  0 114   9   1   5   3   0   1  49   0]\n",
      " [  0   7  82  36   0   1   0   0  51   0]\n",
      " [  0   4   6 115   0   4   0   6  43   5]\n",
      " [  0   0   0   0 128   8   0  42   1   2]\n",
      " [  0   2   0  18   0 148   0   2  10   2]\n",
      " [  0   0   0   0   0   0 181   0   0   0]\n",
      " [  0   1   0   1   0   3   0 171   1   2]\n",
      " [  0   9   8   8   0   1   0   2 144   2]\n",
      " [  0   6   0  41   4   2   0   7  13 107]]\n",
      "error rate: 0.2392877017250974\n",
      "\n",
      "\n",
      "\n",
      "Density Tree with n_min=20\n",
      "[[178   0   0   0   0   0   0   0   0   0]\n",
      " [  0 113   3   0   9   0   0   5  52   0]\n",
      " [  0  25  98   6   0   0   0   0  48   0]\n",
      " [  0   7   2 118   0   4   0   6  42   4]\n",
      " [  0   1   0   0 152   2   0  23   0   3]\n",
      " [  0   1   0  21   0 121   0  16  14   9]\n",
      " [  0   0   0   0   0   0 181   0   0   0]\n",
      " [  0   0   0   0   1   0   0 177   1   0]\n",
      " [  0  22   0   0   1   0   0   5 145   1]\n",
      " [  0   6   0  41   7   3   0  14  14  95]]\n",
      "error rate: 0.23316638842515303\n",
      "\n",
      "\n",
      "\n",
      "Density Tree with n_min=10\n",
      "[[178   0   0   0   0   0   0   0   0   0]\n",
      " [  0 127  10   0   1   3   0   7  30   4]\n",
      " [  0   6  85  24   0   2   0   0  60   0]\n",
      " [  0   2   1 109   0   3   0   5  61   2]\n",
      " [  0   2   0   0 137   0   0  37   1   4]\n",
      " [  0   0   0  15   0 146   0   5  13   3]\n",
      " [  0   1   0   0   0   0 180   0   0   0]\n",
      " [  0   0   0   0   2   1   0 173   2   1]\n",
      " [  0   4   1  11   0   6   0   4 147   1]\n",
      " [  0   3   0  30   2   1   0   6  21 117]]\n",
      "error rate: 0.22148024485253204\n",
      "\n",
      "\n",
      "\n",
      "Density Tree with n_min=5\n",
      "[[177   0   0   1   0   0   0   0   0   0]\n",
      " [  0 133   3   0   4   0   0   5  34   3]\n",
      " [  0  11 112  32   0   1   0   0  20   1]\n",
      " [  0   2   1 127   0   1   0   3  38  11]\n",
      " [  0   0   0   0 160   4   0  14   0   3]\n",
      " [  0   0   0   8   0 160   0   0  10   4]\n",
      " [  0   0   0   0   0   0 180   0   1   0]\n",
      " [  0   0   0   0   2   1   0 173   2   1]\n",
      " [  0   3   0   5   1   1   0   6 157   1]\n",
      " [  0   3   0  18   2   1   0  10  20 126]]\n",
      "error rate: 0.16249304396215913\n",
      "\n",
      "\n",
      "\n",
      "Density Tree with n_min=1\n",
      "[[178   0   0   0   0   0   0   0   0   0]\n",
      " [  0 137   1   0   2   0   0   0  41   1]\n",
      " [  0   8 129   9   0   0   0   0  31   0]\n",
      " [  0   2   1 116   0  12   0   5  46   1]\n",
      " [  0   1   0   0 156   2   0  22   0   0]\n",
      " [  0   1   0  13   0 151   0   2  11   4]\n",
      " [  0   0   0   0   0   0 181   0   0   0]\n",
      " [  0   1   0   0   0   0   0 175   3   0]\n",
      " [  0   2   1   1   0   1   0   5 164   0]\n",
      " [  0   2   0  18   2   9   0   6  17 126]]\n",
      "error rate: 0.15804117974401777\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_density_tree(40)\n",
    "test_density_tree(20)\n",
    "test_density_tree(10)\n",
    "test_density_tree(5)\n",
    "test_density_tree(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree with n_min=40\n",
      "[[152   1   0   0   8   1   2  12   0   2]\n",
      " [  0 137   4  12   9   0   4   5   6   5]\n",
      " [  0   7 144   5   1   0   2   5   8   5]\n",
      " [  0   2  10 136   1   3   0  12   9  10]\n",
      " [  4   3   0   0 155   1   0  12   1   5]\n",
      " [  0   5   4   2  11 149   0   3   4   4]\n",
      " [  1   0   0   0   5   1 166   3   2   3]\n",
      " [  0   3   3   0   6   2   0 153   9   3]\n",
      " [  0  10  11  13   3   4   0  11 108  14]\n",
      " [  0   6   8   5   0   2   0   7   0 152]]\n",
      "error rate: 0.19198664440734559\n",
      "\n",
      "\n",
      "\n",
      "Decision Tree with n_min=20\n",
      "[[171   0   0   0   1   2   1   0   2   1]\n",
      " [  2 149   2   3   2   4   5   0  11   4]\n",
      " [  3  12 148   1   0   0   1   0   9   3]\n",
      " [  0   7   0 151   0   0   1   5  12   7]\n",
      " [  6   2   1   0 158   9   2   3   0   0]\n",
      " [  2   5   0   1   3 156   6   5   2   2]\n",
      " [  1   3   0   0   0   3 172   1   1   0]\n",
      " [  1   3   0   2   5   5   0 161   1   1]\n",
      " [  1  12   4   6   0   0   6   5 135   5]\n",
      " [  4   3   0   2   1  10   1   2   3 154]]\n",
      "error rate: 0.1346688925987758\n",
      "\n",
      "\n",
      "\n",
      "Decision Tree with n_min=10\n",
      "[[173   1   0   0   1   0   1   1   0   1]\n",
      " [  0 164   2   6   0   1   2   2   3   2]\n",
      " [  1   1 165   4   0   0   0   3   3   0]\n",
      " [  0   2   0 173   1   3   0   1   3   0]\n",
      " [  0   3   0   0 172   0   1   1   2   2]\n",
      " [  0   2   3   1   1 173   0   1   1   0]\n",
      " [  2   4   0   0   1   1 171   0   0   2]\n",
      " [  0   2   0   3   0   0   1 170   1   2]\n",
      " [  0   4   1   6   1   0   1   3 157   1]\n",
      " [  0   1   1   3   0   2   0   4   1 168]]\n",
      "error rate: 0.06176961602671116\n",
      "\n",
      "\n",
      "\n",
      "Decision Tree with n_min=5\n",
      "[[175   0   0   0   0   0   0   3   0   0]\n",
      " [  0 174   4   1   0   1   0   0   1   1]\n",
      " [  0   1 171   2   0   1   1   0   1   0]\n",
      " [  0   0   0 179   0   1   0   0   2   1]\n",
      " [  0   0   0   0 179   0   1   1   0   0]\n",
      " [  0   0   0   1   3 174   2   0   1   1]\n",
      " [  0   0   0   0   1   0 180   0   0   0]\n",
      " [  0   0   1   1   3   0   0 171   1   2]\n",
      " [  0   0   0   4   0   0   2   1 166   1]\n",
      " [  0   0   0   3   0   0   1   0   3 173]]\n",
      "error rate: 0.03060656649972171\n",
      "\n",
      "\n",
      "\n",
      "Decision Tree with n_min=1\n",
      "[[178   0   0   0   0   0   0   0   0   0]\n",
      " [  0 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0 177   0   0   0   0   0   0   0]\n",
      " [  0   0   0 183   0   0   0   0   0   0]\n",
      " [  0   0   0   0 181   0   0   0   0   0]\n",
      " [  0   0   0   0   0 182   0   0   0   0]\n",
      " [  0   0   0   0   0   0 181   0   0   0]\n",
      " [  0   0   0   0   0   0   0 179   0   0]\n",
      " [  0   0   0   0   0   0   0   0 174   0]\n",
      " [  0   0   0   0   0   0   0   0   0 180]]\n",
      "error rate: 0.0\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_decision_tree(40)\n",
    "test_decision_tree(20)\n",
    "test_decision_tree(10)\n",
    "test_decision_tree(5)\n",
    "test_decision_tree(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comment\n",
    "The error rate of training set will decline with smaller n_min.  \n",
    "Meanwhile, decision tree algorithm performs much better than density tree in terms of the error rate of training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density and Decision Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DensityForest():\n",
    "    def __init__(self, n_trees):\n",
    "        # create ensemble\n",
    "        self.trees = [DensityTree() for i in range(n_trees)]\n",
    "    \n",
    "    def train(self, data, prior, n_min=20):\n",
    "        for tree in self.trees:\n",
    "            # train each tree, using a bootstrap sample of the data\n",
    "            tree.train(data[np.random.choice(len(data), size=len(data))], prior, n_min) # your code here\n",
    "\n",
    "    def predict(self, x):\n",
    "        # compute the ensemble prediction\n",
    "        return np.mean(np.array([tree.predict(x) for tree in self.trees])) # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DecisionForest():\n",
    "    def __init__(self, n_trees):\n",
    "        # create ensemble\n",
    "        self.trees = [DecisionTree() for i in range(n_trees)]\n",
    "    \n",
    "    def train(self, data, labels, n_min=0):\n",
    "        for tree in self.trees:\n",
    "            # train each tree, using a bootstrap sample of the data\n",
    "            indices = np.random.choice(len(data), size=len(data))\n",
    "            tree.train(data[indices], labels[indices], n_min) # your code here\n",
    "\n",
    "    def predict(self, x):\n",
    "        # compute the ensemble prediction\n",
    "        dicts = [tree.predict(x) for tree in self.trees]\n",
    "        ret = dict()\n",
    "        for prob_dict in dicts:\n",
    "            for label, prob in prob_dict.items():\n",
    "                if label not in ret:\n",
    "                    ret[label] = prob\n",
    "                else:\n",
    "                    ret[label] += prob\n",
    "        # calculate the mean\n",
    "        for label in ret:\n",
    "            ret[label] = ret[label]/len(ret)\n",
    "        return ret # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Density and Decision Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density Forest with n_min=20:\n",
      "[[178   0   0   0   0   0   0   0   0   0]\n",
      " [  0 172   1   0   1   0   0   0   6   2]\n",
      " [  0   6 152   6   0   0   0   0  13   0]\n",
      " [  0   0   1 161   0   0   0   4  12   5]\n",
      " [  0   0   0   0 177   0   0   4   0   0]\n",
      " [  0   0   0  10   0 157   0   1   6   8]\n",
      " [  0   0   0   0   0   0 181   0   0   0]\n",
      " [  0   0   0   1   1   0   0 177   0   0]\n",
      " [  0  16   0   5   0   0   0   5 148   0]\n",
      " [  0   4   0  21   4   1   0  12  11 127]]\n",
      "error rate: 0.09293266555370061\n"
     ]
    }
   ],
   "source": [
    "# train forests (with 20 trees per forest), plot training error confusion matrices, and comment on your results\n",
    "density_forests = [DensityForest(20) for _ in range(10)]\n",
    "for i in range(10):\n",
    "    density_forests[i].train(data=digits.data[digits.target==i], prior=np.sum(digits.target==i)/len(digits.target), n_min=20)\n",
    "confusion_matrice = np.zeros((10, 10), dtype=int)\n",
    "for i in range(len(digits.target)):\n",
    "    x = digits.data[i]\n",
    "    real_label = digits.target[i]\n",
    "    pred_label = np.argsort([density_forests[i].predict(x) for i in range(10)])[-1]\n",
    "    confusion_matrice[real_label][pred_label] += 1\n",
    "print(\"Density Forest with n_min=20:\")\n",
    "print(confusion_matrice)\n",
    "print(\"error rate:\", cal_err(confusion_matrice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Forest with n_min=20:\n",
      "[[178   0   0   0   0   0   0   0   0   0]\n",
      " [  0 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0 177   0   0   0   0   0   0   0]\n",
      " [  0   0   0 181   0   2   0   0   0   0]\n",
      " [  0   0   0   0 180   0   0   1   0   0]\n",
      " [  0   0   0   0   0 181   0   0   0   1]\n",
      " [  0   0   0   0   0   0 181   0   0   0]\n",
      " [  0   0   0   0   0   0   0 179   0   0]\n",
      " [  0   0   0   1   0   0   0   0 173   0]\n",
      " [  0   0   0   0   0   0   0   1   1 178]]\n",
      "error rate: 0.0038953811908736258\n"
     ]
    }
   ],
   "source": [
    "decision_forest = DecisionForest(20)\n",
    "decision_forest.train(data=digits.data, labels=digits.target, n_min=20)\n",
    "confusion_matrice = np.zeros((10, 10), dtype=int)\n",
    "for i in range(len(digits.target)):\n",
    "    x = digits.data[i]\n",
    "    real_label = digits.target[i]\n",
    "    pred_label = 0\n",
    "    max_prob = 0\n",
    "    for label, p in decision_forest.predict(x).items():\n",
    "        if p > max_prob:\n",
    "            pred_label = label\n",
    "            max_prob = p\n",
    "#     print(\"real:\", real_label)\n",
    "#     print(\"pred:\", [density_trees[i].predict(x) for i in range(10)])\n",
    "    confusion_matrice[real_label][pred_label] += 1\n",
    "    \n",
    "print(\"Decision Forest with n_min=20:\")\n",
    "print(confusion_matrice)\n",
    "print(\"error rate:\", cal_err(confusion_matrice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier with n_min=20:\n",
      "[[177   0   0   0   1   0   0   0   0   0]\n",
      " [  0 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0 177   0   0   0   0   0   0   0]\n",
      " [  1   0   1 179   0   0   0   1   1   0]\n",
      " [  0   0   0   0 180   0   0   0   1   0]\n",
      " [  0   0   0   1   1 179   1   0   0   0]\n",
      " [  1   0   0   0   0   0 179   0   1   0]\n",
      " [  0   0   0   0   0   0   0 179   0   0]\n",
      " [  0   2   0   0   0   0   0   0 172   0]\n",
      " [  0   0   0   1   0   0   0   1   1 177]]\n",
      "error rate: 0.008903728436282732\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "rfc = RandomForestClassifier(20, min_samples_split = 20)\n",
    "rfc.fit(digits.data, digits.target)\n",
    "confusion_matrice = np.zeros((10, 10), dtype=int)\n",
    "for i in range(len(digits.target)):\n",
    "    x = digits.data[i]\n",
    "    real_label = digits.target[i]\n",
    "    pred_label = rfc.predict([x])\n",
    "    confusion_matrice[real_label][pred_label] += 1\n",
    "    \n",
    "print(\"RandomForestClassifier with n_min=20:\")\n",
    "print(confusion_matrice)\n",
    "print(\"error rate:\", cal_err(confusion_matrice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comment\n",
    "The forest algorithm is much better than the respective tree alogrithm.  \n",
    "And our Decision Forest achieves similar accuracy with RandomForestClassifier from sklearn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
