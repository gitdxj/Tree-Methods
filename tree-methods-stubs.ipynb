{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "... # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base classes\n",
    "\n",
    "class Node:\n",
    "    pass\n",
    "\n",
    "class Tree:\n",
    "    def __init__(self):\n",
    "        self.root = Node()\n",
    "    \n",
    "    def find_leaf(self, x):\n",
    "        node = self.root\n",
    "        while hasattr(node, \"feature\"):\n",
    "            j = node.feature\n",
    "            if x[j] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DensityTree(Tree):\n",
    "    def __init__(self):\n",
    "        super(DensityTree, self).__init__()\n",
    "        \n",
    "    def train(self, data, prior, n_min=20):\n",
    "        '''\n",
    "        data: the feature matrix for the digit under consideration\n",
    "        prior: the prior probability of this digit\n",
    "        n_min: termination criterion (don't split if a node contains fewer instances)\n",
    "        '''\n",
    "        self.prior = prior\n",
    "        N, D = data.shape\n",
    "        D_try = int(np.sqrt(D)) # number of features to consider for each split decision\n",
    "\n",
    "        # find and remember the tree's bounding box, \n",
    "        # i.e. the lower and upper limits of the training feature set\n",
    "        m, M = np.min(data, axis=0), np.max(data, axis=0)\n",
    "        self.box = m.copy(), M.copy()\n",
    "        \n",
    "#         print(\"shape of m and M\", m.shape, M.shape)\n",
    "        \n",
    "        # identify invalid features and adjust the bounding box\n",
    "        # (If m[j] == M[j] for some j, the bounding box has zero volume, \n",
    "        #  causing divide-by-zero errors later on. We must exclude these\n",
    "        #  features from splitting and adjust the bounding box limits \n",
    "        #  such that invalid features have no effect on the volume.)\n",
    "        valid_features   = np.where(m != M)[0]\n",
    "        invalid_features = np.where(m == M)[0]\n",
    "        M[invalid_features] = m[invalid_features] + 1\n",
    "        \n",
    "#         print(\"shape of valid features\", valid_features.shape)\n",
    "\n",
    "        # initialize the root node\n",
    "        self.root.data = data\n",
    "        self.root.box = m.copy(), M.copy()\n",
    "\n",
    "        # build the tree\n",
    "        stack = [self.root]\n",
    "        while len(stack):\n",
    "            node = stack.pop()\n",
    "            n = node.data.shape[0] # number of instances in present node\n",
    "            if n >= n_min:\n",
    "                # Call 'make_density_split_node()' with 'D_try' randomly selected \n",
    "                # indices from 'valid_features'. This turns 'node' into a split node\n",
    "                # and returns the two children, which must be placed on the 'stack'.\n",
    "                left, right = make_density_split_node(node, N, np.random.permutation(valid_features)[:D_try])\n",
    "                if left==None or right==None:\n",
    "                    make_density_leaf_node(node, N)\n",
    "                    if len(stack):\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "                stack.append(left)\n",
    "                stack.append(right)\n",
    "            else:\n",
    "                # Call 'make_density_leaf_node()' to turn 'node' into a leaf node.\n",
    "                make_density_leaf_node(node, N)\n",
    "\n",
    "    def predict(self, x):        \n",
    "        # return p(x | y) * p(y) if x is within the tree's bounding box \n",
    "        # and return 0 otherwise\n",
    "        leaf = self.find_leaf(x)\n",
    "        m, M = self.root.box\n",
    "        if np.sum(x<m)>0 or np.sum(x>M)>0:  # out of the bounding box\n",
    "            return 0\n",
    "        else:\n",
    "            return self.prior * leaf.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_density_split_node(node, N, feature_indices):\n",
    "    '''\n",
    "    node: the node to be split\n",
    "    N:    the total number of training instances for the current class\n",
    "    feature_indices: a numpy array of length 'D_try', containing the feature \n",
    "                     indices to be considered in the present split\n",
    "    '''\n",
    "    n, D = node.data.shape\n",
    "    m, M = node.box\n",
    "\n",
    "    # find best feature j (among 'feature_indices') and best threshold t for the split\n",
    "    e_min = float(\"inf\")\n",
    "    j_min, t_min = None, None\n",
    "    \n",
    "    for j in feature_indices:\n",
    "        # Hint: For each feature considered, first remove duplicate feature values using \n",
    "        # 'np.unique()'. Describe here why this is necessary.\n",
    "        \"\"\"if we don't use np.unique(), we will end up calculating the error of some identical thresholds for multiple times\"\"\"\n",
    "        data_unique = np.sort(np.unique(node.data[:, j]))\n",
    "        # Compute candidate thresholds\n",
    "        tj = np.array([(data_unique[i]+data_unique[i+1])/2 for i in range(len(data_unique)-1)])\n",
    "#         print(tj)\n",
    "        # Illustration: for loop - hint: vectorized version is possible\n",
    "        for t in tj:\n",
    "            # Compute the error\n",
    "            Nl = np.sum(node.data[:, j]<=t)\n",
    "            Nr = np.sum(node.data[:, j]>t)\n",
    "#             Vl = np.prod(np.max(node.data[node.data[:, j]<=t], axis=0) - np.min(node.data[node.data[:, j]<=t], axis=0))\n",
    "#             Vr = np.prod(np.max(node.data[node.data[:, j]>t], axis=0) - np.min(node.data[node.data[:, j]>t], axis=0))\n",
    "            V = np.prod(M-m)\n",
    "            Vl = V*(t - m[j])/(M[j] - m[j])\n",
    "            Vr = V*(M[j] - t)/(M[j] - m[j])\n",
    "            loo_error = cal_loo_error(N, Nl, Vl) + cal_loo_error(N, Nr, Vr)\n",
    "            \n",
    "            # choose the best threshold that\n",
    "            if loo_error < e_min:\n",
    "                e_min = loo_error\n",
    "                j_min = j\n",
    "                t_min = t\n",
    "    \n",
    "    if j_min == None:\n",
    "        return None, None\n",
    "    \n",
    "    # create children\n",
    "    left = Node()\n",
    "    right = Node()\n",
    "    \n",
    "    # initialize 'left' and 'right' with the data subsets and bounding boxes\n",
    "    # according to the optimal split found above\n",
    "    left.data = node.data[node.data[:, j_min]<=t_min] # store data in left node -- for subsequent splits\n",
    "    left.box = m.copy(), M.copy()\n",
    "    left.box[1][j_min] = t_min   # store bounding box in left node\n",
    "    right.data = node.data[node.data[:, j_min]>t_min]\n",
    "    right.box = m.copy(), M.copy()\n",
    "    right.box[0][j_min] = t_min\n",
    "\n",
    "    # turn the current 'node' into a split node\n",
    "    # (store children and split condition)\n",
    "    node.left = left\n",
    "    node.right = right\n",
    "    node.feature = j_min\n",
    "    node.threshold = t_min\n",
    "\n",
    "    # return the children (to be placed on the stack)\n",
    "    return left, right\n",
    "\n",
    "\n",
    "def cal_loo_error(N, Nm, Vm):\n",
    "    return (Nm/(N*Vm))*(Nm/N - 2*(Nm-1)/(N-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_density_leaf_node(node, N):\n",
    "    '''\n",
    "    node: the node to become a leaf\n",
    "    N:    the total number of training instances for the current class\n",
    "    '''\n",
    "    # compute and store leaf response\n",
    "    n = node.data.shape[0]\n",
    "    v = np.prod(node.box[1]-node.box[0])\n",
    "    node.response = n/(N*v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DecisionTree(Tree):\n",
    "    def __init__(self):\n",
    "        super(DecisionTree, self).__init__()\n",
    "        \n",
    "    def train(self, data, labels, n_min=20):\n",
    "        '''\n",
    "        data: the feature matrix for all digits\n",
    "        labels: the corresponding ground-truth responses\n",
    "        n_min: termination criterion (don't split if a node contains fewer instances)\n",
    "        '''\n",
    "        N, D = data.shape\n",
    "        D_try = int(np.sqrt(D)) # how many features to consider for each split decision\n",
    "\n",
    "        # initialize the root node\n",
    "        self.root.data = data\n",
    "        self.root.labels = labels\n",
    "        \n",
    "        stack = [self.root]\n",
    "        while len(stack):\n",
    "            node = stack.pop()\n",
    "            n = node.data.shape[0] # number of instances in present node\n",
    "            if n >= n_min and not node_is_pure(node):\n",
    "                # Call 'make_decision_split_node()' with 'D_try' randomly selected \n",
    "                # feature indices. This turns 'node' into a split node\n",
    "                # and returns the two children, which must be placed on the 'stack'.\n",
    "                left, right = make_decision_split_node(node, np.random.permutation(node.data.shape[1])[:D_try])\n",
    "                if left==None or right==None:\n",
    "                    make_decision_leaf_node(node)\n",
    "                    if len(stack):\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "                stack.append(left)\n",
    "                stack.append(right)\n",
    "                ... # your code here\n",
    "            else:\n",
    "                # Call 'make_decision_leaf_node()' to turn 'node' into a leaf node.\n",
    "                make_decision_leaf_node(node)\n",
    "                ... # your code here\n",
    "                \n",
    "    def predict(self, x):\n",
    "        leaf = self.find_leaf(x)\n",
    "        # compute p(y | x)\n",
    "        return leaf.response # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_decision_split_node(node, feature_indices):\n",
    "    '''\n",
    "    node: the node to be split\n",
    "    feature_indices: a numpy array of length 'D_try', containing the feature \n",
    "                     indices to be considered in the present split\n",
    "    '''\n",
    "    n, D = node.data.shape\n",
    "\n",
    "    # find best feature j (among 'feature_indices') and best threshold t for the split\n",
    "    e_min = float(\"inf\")\n",
    "    j_min, t_min = None, None\n",
    "    \n",
    "    for j in feature_indices:\n",
    "        data_unique = np.sort(np.unique(node.data[:, j]))\n",
    "        # Compute candidate thresholds\n",
    "        tj = np.array([(data_unique[i]+data_unique[i+1])/2 for i in range(len(data_unique)-1)])\n",
    "        \n",
    "        # Illustration: for loop - hint: vectorized version is possible\n",
    "        for t in tj:\n",
    "            # Compute the error\n",
    "            Nl = np.sum(node.data[:, j]<=t)\n",
    "            Nr = np.sum(node.data[:, j]>t)\n",
    "            gini = cal_gini(Nl, node.labels, node.data[:, j]<=t) + cal_gini(Nr, node.labels, node.data[:, j]>t)\n",
    "            \n",
    "            # choose the best threshold that\n",
    "            if gini < e_min:\n",
    "                e_min = gini\n",
    "                j_min = j\n",
    "                t_min = t\n",
    "    \n",
    "    if j_min == None:\n",
    "        return None, None\n",
    "\n",
    "    # create children\n",
    "    left = Node()\n",
    "    right = Node()\n",
    "    \n",
    "    # initialize 'left' and 'right' with the data subsets and labels\n",
    "    # according to the optimal split found above\n",
    "    left.data = node.data[node.data[:, j_min]<=t_min] # data in left node\n",
    "    left.labels = node.labels[node.data[:, j_min]<=t_min] # corresponding labels\n",
    "    right.data = node.data[node.data[:, j_min]>t_min]\n",
    "    right.labels = node.labels[node.data[:, j_min]>t_min]\n",
    "\n",
    "\n",
    "    # turn the current 'node' into a split node\n",
    "    # (store children and split condition)\n",
    "    node.left = left\n",
    "    node.right = right\n",
    "    node.feature = j_min\n",
    "    node.threshold = t_min\n",
    "\n",
    "    # return the children (to be placed on the stack)\n",
    "    return left, right    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_decision_leaf_node(node):\n",
    "    '''\n",
    "    node: the node to become a leaf\n",
    "    '''\n",
    "    \n",
    "    # compute and store leaf response\n",
    "    node.N = len(node.labels)\n",
    "    response = dict()   # Key: label  Value: K/N\n",
    "    for label in node.labels:\n",
    "        if label not in response:\n",
    "            response[label] = 1\n",
    "        else:\n",
    "            response[label] += 1\n",
    "    for k in response:\n",
    "        response[k] = response[k] / node.N\n",
    "    node.response = response # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def node_is_pure(node):\n",
    "    '''\n",
    "    check if 'node' ontains only instances of the same digit\n",
    "    '''\n",
    "    return len(np.unique(node.labels))==1 # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_gini(Nl, labels, indices):\n",
    "    \"\"\"\n",
    "    Calculation of Gini impurity\n",
    "    Args:\n",
    "        Nl: number of instances in node l\n",
    "        labels: labels of node l's parent\n",
    "        indices: use labels[indices] to get the labels of node l\n",
    "    \"\"\"\n",
    "    label_count = dict()  # key: label  value: count of the label\n",
    "    for label in labels[indices]:\n",
    "        if label not in label_count:\n",
    "            label_count[label] = 1\n",
    "        else:\n",
    "            label_count[label] += 1\n",
    "    gini = 0\n",
    "    for _, Nlk in label_count.items():\n",
    "        gini += np.square(Nlk/Nl)\n",
    "    gini = Nl*(1-gini)\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Density and Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'target_names', 'images', 'DESCR'])\n",
      "(1797, 8, 8)\n",
      "(1797,)\n"
     ]
    }
   ],
   "source": [
    "# read and prepare the digits data\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print(digits.keys())\n",
    "print(digits.images.shape)\n",
    "print(digits.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_err(mat):\n",
    "    assert mat.shape[0] == mat.shape[1]\n",
    "    D = mat.shape[0]\n",
    "    N = np.sum(np.sum(mat, 0))  # number of instances\n",
    "    correct_pred_count = 0\n",
    "    for i in range(D):\n",
    "        correct_pred_count += mat[i][i]\n",
    "    return 1- correct_pred_count/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# train trees, plot training error confusion matrices, and comment on your results\n",
    "def test_density_tree(n_min = 20):\n",
    "    density_trees = [DensityTree() for _ in range(10)]\n",
    "    for i in range(10):\n",
    "        density_trees[i].train(data=digits.data[digits.target==i], prior=np.sum(digits.target==i)/len(digits.target), n_min=n_min)\n",
    "    confusion_matrice = np.zeros((10, 10), dtype=int)\n",
    "    for i in range(len(digits.target)):\n",
    "        x = digits.data[i]\n",
    "        real_label = digits.target[i]\n",
    "        pred_label = np.argsort([density_trees[i].predict(x) for i in range(10)])[-1]\n",
    "        confusion_matrice[real_label][pred_label] += 1\n",
    "    print(\"Density Tree with n_min={}\".format(n_min))\n",
    "    print(confusion_matrice)\n",
    "    print(\"error rate:\", cal_err(confusion_matrice))\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_decision_tree(n_min = 20):\n",
    "    decision_tree = DecisionTree()\n",
    "    decision_tree.train(data=digits.data, labels=digits.target, n_min=n_min)\n",
    "    confusion_matrice = np.zeros((10, 10), dtype=int)\n",
    "    for i in range(len(digits.target)):\n",
    "        x = digits.data[i]\n",
    "        real_label = digits.target[i]\n",
    "        pred_label = 0\n",
    "        max_prob = 0\n",
    "        for label, p in decision_tree.predict(x).items():\n",
    "            if p > max_prob:\n",
    "                pred_label = label\n",
    "                max_prob = p\n",
    "        confusion_matrice[real_label][pred_label] += 1\n",
    "    print(\"Decision Tree with n_min={}\".format(n_min))\n",
    "    print(confusion_matrice)\n",
    "    print(\"error rate:\", cal_err(confusion_matrice))\n",
    "    print(\"\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density Tree with n_min=40\n",
      "[[178   0   0   0   0   0   0   0   0   0]\n",
      " [  0 121   7   0   4   7   0   5  38   0]\n",
      " [  0  10  98   4   0   0   0   0  62   3]\n",
      " [  0   1   4 121   0   8   0   5  37   7]\n",
      " [  0   2   0   0 128   4   0  46   0   1]\n",
      " [  0   0   0   8   2 152   0   3  11   6]\n",
      " [  0   0   0   0   0   0 180   0   1   0]\n",
      " [  0   0   0   0   3   0   0 174   2   0]\n",
      " [  0   5   2   0   1   6   0   5 155   0]\n",
      " [  0   5   0  32   6   1   0  13  11 112]]\n",
      "error rate: 0.21035058430717868\n",
      "\n",
      "\n",
      "\n",
      "Density Tree with n_min=20\n",
      "[[177   0   0   0   0   0   0   0   1   0]\n",
      " [  0 126  11   0   2   2   0   3  27  11]\n",
      " [  0   9 104  10   0   4   0   0  50   0]\n",
      " [  0   2   7 132   0   1   0   5  26  10]\n",
      " [  0   1   0   0 143   0   0  32   2   3]\n",
      " [  0   0   0  22   0 134   0   4  20   2]\n",
      " [  0   0   0   0   0   0 180   0   1   0]\n",
      " [  0   0   0   1   2   1   0 174   1   0]\n",
      " [  0   9   5   3   0   2   0  10 145   0]\n",
      " [  0   6   2  42   2   2   0   6  18 102]]\n",
      "error rate: 0.21146355036171394\n",
      "\n",
      "\n",
      "\n",
      "Density Tree with n_min=10\n",
      "[[178   0   0   0   0   0   0   0   0   0]\n",
      " [  0 138  12   0   3   1   0   1  27   0]\n",
      " [  0   3  86  21   0   0   0   0  65   2]\n",
      " [  0   2   6 133   0   6   0   5  29   2]\n",
      " [  0   4   0   0 149   1   0  27   0   0]\n",
      " [  0   0   0  17   3 152   0   3   6   1]\n",
      " [  0   0   0   0   0   0 180   0   1   0]\n",
      " [  0   0   0   1   3   2   0 171   2   0]\n",
      " [  0  11   2   2   0   0   0   6 153   0]\n",
      " [  0   4   0  47   6   3   0   7  24  89]]\n",
      "error rate: 0.20478575403450194\n",
      "\n",
      "\n",
      "\n",
      "Density Tree with n_min=5\n",
      "[[176   1   0   0   1   0   0   0   0   0]\n",
      " [  0 139   7   0   8   4   0   1  23   0]\n",
      " [  0   6 130  11   0   1   0   0  29   0]\n",
      " [  0   5   1 103   0  10   0   5  45  14]\n",
      " [  0   0   0   0 167   2   0  11   0   1]\n",
      " [  0   0   0   7   0 145   0   6  16   8]\n",
      " [  0   1   0   0   0   1 179   0   0   0]\n",
      " [  0   0   0   1   6   1   0 167   4   0]\n",
      " [  0  13   4   0   1   5   0   2 149   0]\n",
      " [  0   5   0  25   2   6   0   6  19 117]]\n",
      "error rate: 0.18085698386199223\n",
      "\n",
      "\n",
      "\n",
      "Density Tree with n_min=1\n",
      "[[178   0   0   0   0   0   0   0   0   0]\n",
      " [  0 137  14   1   3   2   0   2  21   2]\n",
      " [  0   3 105  35   0   1   0   0  33   0]\n",
      " [  0   2  14 114   0   1   0   5  42   5]\n",
      " [  0   1   0   0 165   3   0  11   0   1]\n",
      " [  0   1   0   9   0 153   0   0   8  11]\n",
      " [  0   0   0   0   0   1 180   0   0   0]\n",
      " [  0   0   0   1   2   2   0 171   3   0]\n",
      " [  0  14   5   2   2   5   0   3 143   0]\n",
      " [  0   5   2  29   1   1   0   7  18 117]]\n",
      "error rate: 0.18586533110740122\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_density_tree(40)\n",
    "test_density_tree(20)\n",
    "test_density_tree(10)\n",
    "test_density_tree(5)\n",
    "test_density_tree(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree with n_min=40\n",
      "[[164   8   0   0   3   0   3   0   0   0]\n",
      " [  0 136  15   0   1   0   1   1  14  14]\n",
      " [  2   2 153   1   1   0   0   0  15   3]\n",
      " [  0   0   4 142   2   3   0   5  16  11]\n",
      " [  0   8   3   0 153   0   0  12   5   0]\n",
      " [  0   2   0   6   0 158   0   4   2  10]\n",
      " [  0   5   0   0  10   0 150   2   7   7]\n",
      " [  0   1   5   1  11   0   0 151   7   3]\n",
      " [  0  10  25   5   0   2   2   7 120   3]\n",
      " [  2   7   2   2   1   4   1   7   3 151]]\n",
      "error rate: 0.17751808569838623\n",
      "\n",
      "\n",
      "\n",
      "Decision Tree with n_min=20\n",
      "[[172   0   0   2   1   2   0   0   1   0]\n",
      " [  0 155   2   2   8   1   2   5   1   6]\n",
      " [  0   2 158   2   0   0   0   5   6   4]\n",
      " [  0   0   0 168   0   5   0   1   3   6]\n",
      " [  0   0   0   1 172   1   0   3   2   2]\n",
      " [  0   2   4   5   5 152   4   2   4   4]\n",
      " [  0   5   0   0   4   2 167   1   1   1]\n",
      " [  0   1   0   1   3   0   1 167   3   3]\n",
      " [  4   4   0   7   1   9   5   2 136   6]\n",
      " [  4   1   0  19   3   3   0   2   4 144]]\n",
      "error rate: 0.1146355036171397\n",
      "\n",
      "\n",
      "\n",
      "Decision Tree with n_min=10\n",
      "[[174   0   0   0   0   1   0   0   1   2]\n",
      " [  0 169   1   0   3   0   3   0   4   2]\n",
      " [  0   0 167   5   0   1   1   0   2   1]\n",
      " [  0   1   5 164   0   2   0   2   6   3]\n",
      " [  0   2   0   3 165   3   1   0   2   5]\n",
      " [  0   0   2   2   1 172   0   0   4   1]\n",
      " [  2   2   0   3   0   1 169   0   4   0]\n",
      " [  0   2   2   3   2   2   0 163   1   4]\n",
      " [  2   7   0   2   2   7   0   0 150   4]\n",
      " [  0   2   1   2   1   0   0   1   3 170]]\n",
      "error rate: 0.07456872565386752\n",
      "\n",
      "\n",
      "\n",
      "Decision Tree with n_min=5\n",
      "[[176   0   0   0   0   1   1   0   0   0]\n",
      " [  1 174   2   0   0   2   0   2   0   1]\n",
      " [  0   1 171   0   0   0   0   4   1   0]\n",
      " [  0   1   2 172   0   4   0   0   1   3]\n",
      " [  0   1   0   1 175   1   0   2   1   0]\n",
      " [  0   2   0   0   0 177   0   0   2   1]\n",
      " [  0   0   0   0   1   1 179   0   0   0]\n",
      " [  0   0   0   0   1   0   0 177   1   0]\n",
      " [  2   3   0   3   0   0   1   0 165   0]\n",
      " [  1   1   0   0   1   2   0   1   0 174]]\n",
      "error rate: 0.03171953255425708\n",
      "\n",
      "\n",
      "\n",
      "Decision Tree with n_min=1\n",
      "[[178   0   0   0   0   0   0   0   0   0]\n",
      " [  0 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0 177   0   0   0   0   0   0   0]\n",
      " [  0   0   0 183   0   0   0   0   0   0]\n",
      " [  0   0   0   0 181   0   0   0   0   0]\n",
      " [  0   0   0   0   0 182   0   0   0   0]\n",
      " [  0   0   0   0   0   0 181   0   0   0]\n",
      " [  0   0   0   0   0   0   0 179   0   0]\n",
      " [  0   0   0   0   0   0   0   0 174   0]\n",
      " [  0   0   0   0   0   0   0   0   0 180]]\n",
      "error rate: 0.0\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_decision_tree(40)\n",
    "test_decision_tree(20)\n",
    "test_decision_tree(10)\n",
    "test_decision_tree(5)\n",
    "test_decision_tree(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comment\n",
    "The error rate of training set will decline with smaller n_min.  \n",
    "Meanwhile, decision tree algorithm performs much better than density tree in terms of the error rate of training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density and Decision Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DensityForest():\n",
    "    def __init__(self, n_trees):\n",
    "        # create ensemble\n",
    "        self.trees = [DensityTree() for i in range(n_trees)]\n",
    "    \n",
    "    def train(self, data, prior, n_min=20):\n",
    "        for tree in self.trees:\n",
    "            # train each tree, using a bootstrap sample of the data\n",
    "            tree.train(data[np.random.choice(len(data), size=len(data))], prior, n_min) # your code here\n",
    "\n",
    "    def predict(self, x):\n",
    "        # compute the ensemble prediction\n",
    "        return np.mean(np.array([tree.predict(x) for tree in self.trees])) # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DecisionForest():\n",
    "    def __init__(self, n_trees):\n",
    "        # create ensemble\n",
    "        self.trees = [DecisionTree() for i in range(n_trees)]\n",
    "    \n",
    "    def train(self, data, labels, n_min=0):\n",
    "        for tree in self.trees:\n",
    "            # train each tree, using a bootstrap sample of the data\n",
    "            indices = np.random.choice(len(data), size=len(data))\n",
    "            tree.train(data[indices], labels[indices], n_min) # your code here\n",
    "\n",
    "    def predict(self, x):\n",
    "        # compute the ensemble prediction\n",
    "        dicts = [tree.predict(x) for tree in self.trees]\n",
    "        ret = dict()\n",
    "        for prob_dict in dicts:\n",
    "            for label, prob in prob_dict.items():\n",
    "                if label not in ret:\n",
    "                    ret[label] = prob\n",
    "                else:\n",
    "                    ret[label] += prob\n",
    "        # calculate the mean\n",
    "        for label in ret:\n",
    "            ret[label] = ret[label]/len(ret)\n",
    "        return ret # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Density and Decision Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density Forest with n_min=20:\n",
      "[[178   0   0   0   0   0   0   0   0   0]\n",
      " [  0 166   3   0   2   0   0   0  11   0]\n",
      " [  0   5 150   8   0   0   0   0  14   0]\n",
      " [  0   0   1 161   0   0   0   2  19   0]\n",
      " [  0   1   0   0 173   1   0   5   0   1]\n",
      " [  0   0   0   8   0 162   0   1   7   4]\n",
      " [  0   1   0   0   0   0 178   0   2   0]\n",
      " [  0   0   0   0   0   0   0 179   0   0]\n",
      " [  0  12   1   6   0   0   0   0 155   0]\n",
      " [  0   3   0  34   3   1   0   9  21 109]]\n",
      "error rate: 0.10350584307178634\n"
     ]
    }
   ],
   "source": [
    "# train forests (with 20 trees per forest), plot training error confusion matrices, and comment on your results\n",
    "density_forests = [DensityForest(20) for _ in range(10)]\n",
    "for i in range(10):\n",
    "    density_forests[i].train(data=digits.data[digits.target==i], prior=np.sum(digits.target==i)/len(digits.target), n_min=20)\n",
    "confusion_matrice = np.zeros((10, 10), dtype=int)\n",
    "for i in range(len(digits.target)):\n",
    "    x = digits.data[i]\n",
    "    real_label = digits.target[i]\n",
    "    pred_label = np.argsort([density_forests[i].predict(x) for i in range(10)])[-1]\n",
    "    confusion_matrice[real_label][pred_label] += 1\n",
    "print(\"Density Forest with n_min=20:\")\n",
    "print(confusion_matrice)\n",
    "print(\"error rate:\", cal_err(confusion_matrice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Forest with n_min=20:\n",
      "[[178   0   0   0   0   0   0   0   0   0]\n",
      " [  0 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0 177   0   0   0   0   0   0   0]\n",
      " [  0   0   0 182   0   0   0   1   0   0]\n",
      " [  0   0   0   0 181   0   0   0   0   0]\n",
      " [  0   0   0   0   0 181   0   0   0   1]\n",
      " [  0   0   0   0   0   0 181   0   0   0]\n",
      " [  0   0   0   0   0   0   0 178   0   1]\n",
      " [  0   0   0   0   0   0   0   0 174   0]\n",
      " [  0   0   0   1   0   2   0   0   1 176]]\n",
      "error rate: 0.0038953811908736258\n"
     ]
    }
   ],
   "source": [
    "decision_forest = DecisionForest(20)\n",
    "decision_forest.train(data=digits.data, labels=digits.target, n_min=20)\n",
    "confusion_matrice = np.zeros((10, 10), dtype=int)\n",
    "for i in range(len(digits.target)):\n",
    "    x = digits.data[i]\n",
    "    real_label = digits.target[i]\n",
    "    pred_label = 0\n",
    "    max_prob = 0\n",
    "    for label, p in decision_forest.predict(x).items():\n",
    "        if p > max_prob:\n",
    "            pred_label = label\n",
    "            max_prob = p\n",
    "#     print(\"real:\", real_label)\n",
    "#     print(\"pred:\", [density_trees[i].predict(x) for i in range(10)])\n",
    "    confusion_matrice[real_label][pred_label] += 1\n",
    "    \n",
    "print(\"Decision Forest with n_min=20:\")\n",
    "print(confusion_matrice)\n",
    "print(\"error rate:\", cal_err(confusion_matrice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier with n_min=20:\n",
      "[[177   0   0   0   1   0   0   0   0   0]\n",
      " [  0 182   0   0   0   0   0   0   0   0]\n",
      " [  0   1 176   0   0   0   0   0   0   0]\n",
      " [  0   0   0 181   0   1   0   0   1   0]\n",
      " [  0   0   0   0 178   0   0   3   0   0]\n",
      " [  0   0   0   1   0 180   0   0   0   1]\n",
      " [  1   1   0   0   0   0 178   0   1   0]\n",
      " [  0   0   0   0   0   0   0 179   0   0]\n",
      " [  0   1   0   0   0   0   0   0 172   1]\n",
      " [  0   0   0   1   0   2   0   4   1 172]]\n",
      "error rate: 0.012242626599888728\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "rfc = RandomForestClassifier(20, min_samples_split = 20)\n",
    "rfc.fit(digits.data, digits.target)\n",
    "confusion_matrice = np.zeros((10, 10), dtype=int)\n",
    "for i in range(len(digits.target)):\n",
    "    x = digits.data[i]\n",
    "    real_label = digits.target[i]\n",
    "    pred_label = rfc.predict([x])\n",
    "    confusion_matrice[real_label][pred_label] += 1\n",
    "    \n",
    "print(\"RandomForestClassifier with n_min=20:\")\n",
    "print(confusion_matrice)\n",
    "print(\"error rate:\", cal_err(confusion_matrice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comment\n",
    "The forest algorithm is much better than the respective tree alogrithm.  \n",
    "And our Decision Forest achieves similar accuracy with RandomForestClassifier from sklearn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
