{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "... # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base classes\n",
    "\n",
    "class Node:\n",
    "    pass\n",
    "\n",
    "class Tree:\n",
    "    def __init__(self):\n",
    "        self.root = Node()\n",
    "    \n",
    "    def find_leaf(self, x):\n",
    "        node = self.root\n",
    "        while hasattr(node, \"feature\"):\n",
    "            j = node.feature\n",
    "            if x[j] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 0]\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(10)\n",
    "print(np.random.permutation(a[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DensityTree(Tree):\n",
    "    def __init__(self):\n",
    "        super(DensityTree, self).__init__()\n",
    "        \n",
    "    def train(self, data, prior, n_min=20):\n",
    "        '''\n",
    "        data: the feature matrix for the digit under consideration\n",
    "        prior: the prior probability of this digit\n",
    "        n_min: termination criterion (don't split if a node contains fewer instances)\n",
    "        '''\n",
    "        self.prior = prior\n",
    "        N, D = data.shape\n",
    "        D_try = int(np.sqrt(D)) # number of features to consider for each split decision\n",
    "\n",
    "        # find and remember the tree's bounding box, \n",
    "        # i.e. the lower and upper limits of the training feature set\n",
    "        m, M = np.min(data, axis=0), np.max(data, axis=0)\n",
    "        self.box = m.copy(), M.copy()\n",
    "        \n",
    "#         print(\"shape of m and M\", m.shape, M.shape)\n",
    "        \n",
    "        # identify invalid features and adjust the bounding box\n",
    "        # (If m[j] == M[j] for some j, the bounding box has zero volume, \n",
    "        #  causing divide-by-zero errors later on. We must exclude these\n",
    "        #  features from splitting and adjust the bounding box limits \n",
    "        #  such that invalid features have no effect on the volume.)\n",
    "        valid_features   = np.where(m != M)[0]\n",
    "        invalid_features = np.where(m == M)[0]\n",
    "        M[invalid_features] = m[invalid_features] + 1\n",
    "        \n",
    "#         print(\"shape of valid features\", valid_features.shape)\n",
    "\n",
    "        # initialize the root node\n",
    "        self.root.data = data\n",
    "        self.root.box = m.copy(), M.copy()\n",
    "\n",
    "        # build the tree\n",
    "        stack = [self.root]\n",
    "        while len(stack):\n",
    "            node = stack.pop()\n",
    "            n = node.data.shape[0] # number of instances in present node\n",
    "            if n >= n_min:\n",
    "                # Call 'make_density_split_node()' with 'D_try' randomly selected \n",
    "                # indices from 'valid_features'. This turns 'node' into a split node\n",
    "                # and returns the two children, which must be placed on the 'stack'.\n",
    "                left, right = make_density_split_node(node, N, np.random.permutation(valid_features)[:D_try])\n",
    "                stack.append(left)\n",
    "                stack.append(right)\n",
    "            else:\n",
    "                # Call 'make_density_leaf_node()' to turn 'node' into a leaf node.\n",
    "                make_density_leaf_node(node, N)\n",
    "\n",
    "    def predict(self, x):        \n",
    "        # return p(x | y) * p(y) if x is within the tree's bounding box \n",
    "        # and return 0 otherwise\n",
    "        leaf = self.find_leaf(x)\n",
    "        m, M = self.root.box\n",
    "        if np.sum(x<m)>0 or np.sum(x>M)>0:  # out of the bounding box\n",
    "            return 0\n",
    "        else:\n",
    "            return self.prior * leaf.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]),)\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,5])\n",
    "b = np.array([2,3,4])\n",
    "print(np.where(a<b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_density_split_node(node, N, feature_indices):\n",
    "    '''\n",
    "    node: the node to be split\n",
    "    N:    the total number of training instances for the current class\n",
    "    feature_indices: a numpy array of length 'D_try', containing the feature \n",
    "                     indices to be considered in the present split\n",
    "    '''\n",
    "    n, D = node.data.shape\n",
    "    m, M = node.box\n",
    "\n",
    "    # find best feature j (among 'feature_indices') and best threshold t for the split\n",
    "    e_min = float(\"inf\")\n",
    "    j_min, t_min = None, None\n",
    "    \n",
    "    for j in feature_indices:\n",
    "        # Hint: For each feature considered, first remove duplicate feature values using \n",
    "        # 'np.unique()'. Describe here why this is necessary.\n",
    "        \"\"\"if we don't use np.unique(), we will end up calculating the error of some identical thresholds for multiple times\"\"\"\n",
    "        data_unique = np.sort(np.unique(node.data[:, j]))\n",
    "        # Compute candidate thresholds\n",
    "        tj = np.array([(data_unique[i]+data_unique[i+1])/2 for i in range(len(data_unique)-1)])\n",
    "#         print(tj)\n",
    "        # Illustration: for loop - hint: vectorized version is possible\n",
    "        for t in tj:\n",
    "            # Compute the error\n",
    "            Nl = np.sum(node.data[:, j]<=t)\n",
    "            Nr = np.sum(node.data[:, j]>t)\n",
    "#             Vl = np.prod(np.max(node.data[node.data[:, j]<=t], axis=0) - np.min(node.data[node.data[:, j]<=t], axis=0))\n",
    "#             Vr = np.prod(np.max(node.data[node.data[:, j]>t], axis=0) - np.min(node.data[node.data[:, j]>t], axis=0))\n",
    "            V = np.prod(M-m)\n",
    "            Vl = V*(t - m[j])/(M[j] - m[j])\n",
    "            Vr = V*(M[j] - t)/(M[j] - m[j])\n",
    "            loo_error = cal_loo_error(N, Nl, Vl) + cal_loo_error(N, Nr, Vr)\n",
    "            \n",
    "            # choose the best threshold that\n",
    "            if loo_error < e_min:\n",
    "                e_min = loo_error\n",
    "                j_min = j\n",
    "                t_min = t\n",
    "    \n",
    "    # create children\n",
    "    left = Node()\n",
    "    right = Node()\n",
    "    \n",
    "    # initialize 'left' and 'right' with the data subsets and bounding boxes\n",
    "    # according to the optimal split found above\n",
    "    left.data = node.data[node.data[:, j_min]<=t_min] # store data in left node -- for subsequent splits\n",
    "    left.box = m.copy(), M.copy()\n",
    "    left.box[1][j_min] = t_min   # store bounding box in left node\n",
    "    right.data = node.data[node.data[:, j_min]>t_min]\n",
    "    right.box = m.copy(), M.copy()\n",
    "    right.box[0][j_min] = t_min\n",
    "\n",
    "    # turn the current 'node' into a split node\n",
    "    # (store children and split condition)\n",
    "    node.left = left\n",
    "    node.right = right\n",
    "    node.feature = j_min\n",
    "    node.threshold = t_min\n",
    "\n",
    "    # return the children (to be placed on the stack)\n",
    "    return left, right\n",
    "\n",
    "\n",
    "def cal_loo_error(N, Nm, Vm):\n",
    "    return (Nm/(N*Vm))*(Nm/N - 2*(Nm-1)/(N-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_density_leaf_node(node, N):\n",
    "    '''\n",
    "    node: the node to become a leaf\n",
    "    N:    the total number of training instances for the current class\n",
    "    '''\n",
    "    # compute and store leaf response\n",
    "    n = node.data.shape[0]\n",
    "    v = np.prod(node.box[1]-node.box[0])\n",
    "    node.response = n/(N*v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DecisionTree(Tree):\n",
    "    def __init__(self):\n",
    "        super(DecisionTree, self).__init__()\n",
    "        \n",
    "    def train(self, data, labels, n_min=20):\n",
    "        '''\n",
    "        data: the feature matrix for all digits\n",
    "        labels: the corresponding ground-truth responses\n",
    "        n_min: termination criterion (don't split if a node contains fewer instances)\n",
    "        '''\n",
    "        N, D = data.shape\n",
    "        D_try = int(np.sqrt(D)) # how many features to consider for each split decision\n",
    "\n",
    "        # initialize the root node\n",
    "        self.root.data = data\n",
    "        self.root.labels = labels\n",
    "        \n",
    "        stack = [self.root]\n",
    "        while len(stack):\n",
    "            node = stack.pop()\n",
    "            n = node.data.shape[0] # number of instances in present node\n",
    "            if n >= n_min and not node_is_pure(node):\n",
    "                # Call 'make_decision_split_node()' with 'D_try' randomly selected \n",
    "                # feature indices. This turns 'node' into a split node\n",
    "                # and returns the two children, which must be placed on the 'stack'.\n",
    "                left, right = make_decision_split_node(node, np.random.permutation(node.data.shape[1])[:D_try])\n",
    "                stack.append(left)\n",
    "                stack.append(right)\n",
    "                ... # your code here\n",
    "            else:\n",
    "                # Call 'make_decision_leaf_node()' to turn 'node' into a leaf node.\n",
    "                make_decision_leaf_node(node)\n",
    "                ... # your code here\n",
    "                \n",
    "    def predict(self, x):\n",
    "        leaf = self.find_leaf(x)\n",
    "        # compute p(y | x)\n",
    "        return leaf.response # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_decision_split_node(node, feature_indices):\n",
    "    '''\n",
    "    node: the node to be split\n",
    "    feature_indices: a numpy array of length 'D_try', containing the feature \n",
    "                     indices to be considered in the present split\n",
    "    '''\n",
    "    n, D = node.data.shape\n",
    "\n",
    "    # find best feature j (among 'feature_indices') and best threshold t for the split\n",
    "    e_min = float(\"inf\")\n",
    "    j_min, t_min = None, None\n",
    "    \n",
    "    for j in feature_indices:\n",
    "        data_unique = np.sort(np.unique(node.data[:, j]))\n",
    "        # Compute candidate thresholds\n",
    "        tj = np.array([(data_unique[i]+data_unique[i+1])/2 for i in range(len(data_unique)-1)])\n",
    "        \n",
    "        # Illustration: for loop - hint: vectorized version is possible\n",
    "        for t in tj:\n",
    "            # Compute the error\n",
    "            Nl = np.sum(node.data[:, j]<=t)\n",
    "            Nr = np.sum(node.data[:, j]>t)\n",
    "            gini = cal_gini(Nl, node.labels, node.data[:, j]<=t) + cal_gini(Nr, node.labels, node.data[:, j]>t)\n",
    "            \n",
    "            # choose the best threshold that\n",
    "            if gini < e_min:\n",
    "                e_min = gini\n",
    "                j_min = j\n",
    "                t_min = t\n",
    "\n",
    "    # create children\n",
    "    left = Node()\n",
    "    right = Node()\n",
    "    \n",
    "    # initialize 'left' and 'right' with the data subsets and labels\n",
    "    # according to the optimal split found above\n",
    "    left.data = node.data[node.data[:, j_min]<=t_min] # data in left node\n",
    "    left.labels = node.labels[node.data[:, j_min]<=t_min] # corresponding labels\n",
    "    right.data = node.data[node.data[:, j_min]>t_min]\n",
    "    right.labels = node.labels[node.data[:, j_min]>t_min]\n",
    "\n",
    "\n",
    "    # turn the current 'node' into a split node\n",
    "    # (store children and split condition)\n",
    "    node.left = left\n",
    "    node.right = right\n",
    "    node.feature = j_min\n",
    "    node.threshold = t_min\n",
    "\n",
    "    # return the children (to be placed on the stack)\n",
    "    return left, right    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_decision_leaf_node(node):\n",
    "    '''\n",
    "    node: the node to become a leaf\n",
    "    '''\n",
    "    \n",
    "    # compute and store leaf response\n",
    "    node.N = len(node.labels)\n",
    "    response = dict()   # Key: label  Value: K/N\n",
    "    for label in node.labels:\n",
    "        if label not in response:\n",
    "            response[label] = 1\n",
    "        else:\n",
    "            response[label] += 1\n",
    "    for k in response:\n",
    "        response[k] = response[k] / node.N\n",
    "    node.response = response # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def node_is_pure(node):\n",
    "    '''\n",
    "    check if 'node' ontains only instances of the same digit\n",
    "    '''\n",
    "    return len(np.unique(node.labels))==1 # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_gini(Nl, labels, indices):\n",
    "    \"\"\"\n",
    "    Calculation of Gini impurity\n",
    "    Args:\n",
    "        Nl: number of instances in node l\n",
    "        labels: labels of node l's parent\n",
    "        indices: use labels[indices] to get the labels of node l\n",
    "    \"\"\"\n",
    "    label_count = dict()  # key: label  value: count of the label\n",
    "    for label in labels[indices]:\n",
    "        if label not in label_count:\n",
    "            label_count[label] = 1\n",
    "        else:\n",
    "            label_count[label] += 1\n",
    "    gini = 0\n",
    "    for _, Nlk in label_count.items():\n",
    "        gini += np.square(Nlk/Nl)\n",
    "    gini = Nl*(1-gini)\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Density and Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'target_names', 'images', 'DESCR'])\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(1797, 8, 8)\n",
      "(1797,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read and prepare the digits data\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print(digits.keys())\n",
    "print(type(digits.data))\n",
    "print(type(digits.images))\n",
    "print(digits.images.shape)\n",
    "print(digits.target.shape)\n",
    "... # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[178.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0. 139.  15.   0.   3.   1.   0.   0.  18.   6.]\n",
      " [  0.   5. 109.   3.   0.   0.   0.   0.  60.   0.]\n",
      " [  0.   6.  12. 107.   0.  12.   0.   3.  37.   6.]\n",
      " [  0.   1.   0.   0. 152.   9.   0.  19.   0.   0.]\n",
      " [  0.   1.   0.  23.   1. 133.   0.   3.   9.  12.]\n",
      " [  0.   0.   0.   0.   0.   0. 180.   0.   1.   0.]\n",
      " [  0.   0.   0.   0.   6.   3.   0. 169.   1.   0.]\n",
      " [  0.  34.   1.   0.   0.   6.   0.   2. 131.   0.]\n",
      " [  0.   6.   3.  37.   7.   4.   0.  10.  11. 102.]]\n"
     ]
    }
   ],
   "source": [
    "# train trees, plot training error confusion matrices, and comment on your results\n",
    "density_trees = [DensityTree() for _ in range(10)]\n",
    "for i in range(10):\n",
    "    density_trees[i].train(data=digits.data[digits.target==i], prior=np.sum(digits.target==i)/len(digits.target), n_min=20)\n",
    "confusion_matrice = np.zeros((10, 10))\n",
    "for i in range(len(digits.target)):\n",
    "    x = digits.data[i]\n",
    "    real_label = digits.target[i]\n",
    "    pred_label = np.argsort([density_trees[i].predict(x) for i in range(10)])[-1]\n",
    "#     print(\"real:\", real_label)\n",
    "#     print(\"pred:\", [density_trees[i].predict(x) for i in range(10)])\n",
    "    confusion_matrice[real_label][pred_label] += 1\n",
    "print(confusion_matrice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[177.   0.   0.   0.   0.   1.   0.   0.   0.   0.]\n",
      " [  0. 172.   3.   0.   0.   2.   1.   0.   2.   2.]\n",
      " [  0.   0. 174.   1.   0.   0.   0.   0.   1.   1.]\n",
      " [  0.   1.   2. 177.   0.   2.   0.   0.   1.   0.]\n",
      " [  0.   2.   0.   0. 173.   2.   1.   2.   1.   0.]\n",
      " [  1.   1.   0.   1.   1. 175.   1.   0.   2.   0.]\n",
      " [  0.   0.   0.   0.   1.   1. 179.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   1.   0. 177.   0.   1.]\n",
      " [  1.   0.   3.   2.   1.   2.   2.   2. 161.   0.]\n",
      " [  0.   1.   1.   2.   0.   0.   0.   1.   0. 175.]]\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTree()\n",
    "decision_tree.train(data=digits.data, labels=digits.target, n_min=5)\n",
    "confusion_matrice = np.zeros((10, 10))\n",
    "for i in range(len(digits.target)):\n",
    "    x = digits.data[i]\n",
    "    real_label = digits.target[i]\n",
    "    pred_label = 0\n",
    "    max_prob = 0\n",
    "    for label, p in decision_tree.predict(x).items():\n",
    "        if p > max_prob:\n",
    "            pred_label = label\n",
    "            max_prob = p\n",
    "#     print(\"real:\", real_label)\n",
    "#     print(\"pred:\", [density_trees[i].predict(x) for i in range(10)])\n",
    "    confusion_matrice[real_label][pred_label] += 1\n",
    "    \n",
    "print(confusion_matrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = digits.data\n",
    "\n",
    "# N, D = data.shape\n",
    "# D_try = int(np.sqrt(D)) # number of features to consider for each split decision\n",
    "\n",
    "# m, M = np.min(data, axis=0), np.max(data, axis=0)\n",
    "# valid_features   = np.where(m != M)[0]\n",
    "# invalid_features = np.where(m == M)[0]\n",
    "# M[invalid_features] = m[invalid_features] + 1\n",
    "\n",
    "# # initialize the root node\n",
    "# root = Node()\n",
    "# root.data = data\n",
    "# root.box = m.copy(), M.copy()\n",
    "# left, right = make_density_split_node(root, N, valid_features)\n",
    "# print(left.data)\n",
    "# print('\\n')\n",
    "# print(right.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density and Decision Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DensityForest():\n",
    "    def __init__(self, n_trees):\n",
    "        # create ensemble\n",
    "        self.trees = [DensityTree() for i in range(n_trees)]\n",
    "    \n",
    "    def train(self, data, prior, n_min=20):\n",
    "        for tree in self.trees:\n",
    "            # train each tree, using a bootstrap sample of the data\n",
    "            ... # your code here\n",
    "\n",
    "    def predict(self, x):\n",
    "        # compute the ensemble prediction\n",
    "        return ... # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DecisionForest():\n",
    "    def __init__(self, n_trees):\n",
    "        # create ensemble\n",
    "        self.trees = [DecisionTree() for i in range(n_trees)]\n",
    "    \n",
    "    def train(self, data, labels, n_min=0):\n",
    "        for tree in self.trees:\n",
    "            # train each tree, using a bootstrap sample of the data\n",
    "            ... # your code here\n",
    "\n",
    "    def predict(self, x):\n",
    "        # compute the ensemble prediction\n",
    "        return ... # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Density and Decision Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# train forests (with 20 trees per forest), plot training error confusion matrices, and comment on your results\n",
    "... # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
